[{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"the-dataset","dir":"Articles","previous_headings":"","what":"The dataset","title":"Standardise an Event dataset","text":"example, ’ll use dataset frog observations 2015 paper PLOS ONE. Data collected volunteers using 5-minute audio surveys, row documents whether frog species detected 5-minute recording, recorded present (1) absent (0). purpose vignette, downloaded source data Dryad, reduced number rows, converted original excel spreadsheet three .csv files: sites, observations species list.","code":""},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"sites","dir":"Articles","previous_headings":"The dataset","what":"Sites","title":"Standardise an Event dataset","text":"sites spreadsheet contains columns describe survey location (e.g. depth, water_type, latitude, longitude) overall presence/absence frog species site (e.g. cpar, csig, limdum). won’t use aggregated species data stored - ’ll instead export raw observations - ’ll still import data, ’s place spatial information stored.","code":"library(readr) library(readr) library(dplyr) library(tidyr) sites <- read_csv(\"events_sites.csv\")  sites |> rmarkdown::paged_table()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"observations","dir":"Articles","previous_headings":"The dataset","what":"Observations","title":"Standardise an Event dataset","text":"observations spreadsheet contains columns describe sample’s physical properties (e.g. water_type, veg_canopy), linked sites site_code column. importantly, records whether species region recorded particular survey (e.g. cpar, csig, limdum).","code":"obs <- read_csv(\"events_observations.csv\")  obs |> rmarkdown::paged_table()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"species-list","dir":"Articles","previous_headings":"The dataset","what":"Species list","title":"Standardise an Event dataset","text":"Finally, species list spreadsheet lists eight frog species recorded dataset, abbreviation column contains abbreviated column name used observations dataset.","code":"species <- read_csv(\"events_species.csv\")  species #> # A tibble: 8 × 3 #>   scientific_name            common_name            abbreviation #>   <chr>                      <chr>                  <chr>        #> 1 Crinia parinsignifera      Plains Froglet         cpar         #> 2 Crinia signifera           Common Eastern Froglet csig         #> 3 Limnodynastes dumerilii    Pobblebonk             limdum       #> 4 Limnodynastes peronii      Striped Marsh Frog     limper       #> 5 Limnodynastes tasmaniensis Spotted Grass Frog     limtas       #> 6 Litoria peronii            Emerald Spotted Frog   lper         #> 7 Litoria verreauxii         Alpine Tree Frog       lver         #> 8 Uperoleia laevigata        Smooth Toadlet         ulae"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"prepare-events-csv","dir":"Articles","previous_headings":"","what":"Prepare events.csv","title":"Standardise an Event dataset","text":"observations spreadsheet organised sample-level, row contains multiple observations one 5-minute audio recording, can create Event-based dataframe sample-level use events.csv. First, let’s assign unique identifier eventID data, requirement Darwin Core Standard. Using set_events() composite_id(), can create new column eventID containing unique ID constructed several types information dataframe. Next ’ll add site information sites spreadsheet. use set_coordinates() assign existing columns use valid Darwin Core Standard column names, add 2 required columns geodeticDatum coordinateUncertaintyInMetres. now dataframe sampling site information, organised sample-level. final step reduce obs_id_site include columns valid column names Event-based datasets. drops frog species columns dataframe. can specify wish use events Darwin Core Archive use_data(), save events csv file default directory data-publish ./data-publish/events.csv.","code":"obs_id <- obs |>   select(site_code, year, any_of(species$abbreviation)) |>   set_events(     eventID = composite_id(sequential_id(), site_code, year)     ) |>   relocate(eventID, .before = 1) # re-position #> ⠙ Checking 1 column: eventID #> ✔ Checking 1 column: eventID [314ms] #>   obs_id #> # A tibble: 123 × 11 #>    eventID    site_code  year  cpar  csig limdum limper limtas  lper  lver  ulae #>    <chr>      <chr>     <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> #>  1 0001-AMA1… AMA100     2004     1     0      0      0      1     1     0     0 #>  2 0002-AMA1… AMA100     2007     1     0      1      0      1     0     0     0 #>  3 0003-AMA1… AMA100     2007     1     0      1      0      1     0     0     0 #>  4 0004-AMA1… AMA100     2005     1     1      1      0      1     0     0     0 #>  5 0005-AMA1… AMA100     2008     1     0      1      0      0     1     0     0 #>  6 0006-AMA1… AMA100     2008     1     0      1      0      1     1     0     0 #>  7 0007-AMA1… AMA100     2013     1     0      1      0      1     0     0     0 #>  8 0008-AMA1… AMA100     2008     1     0      1      0      1     1     0     0 #>  9 0009-AMA1… AMA100     2013     1     1      0      0      0     0     0     0 #> 10 0010-AMA1… AMA100     2014     1     1      1      0      1     0     0     0 #> # ℹ 113 more rows obs_id_site <- obs_id |>   left_join(     select(sites, site_code, latitude, longitude),     join_by(site_code)     ) |>   set_coordinates(     decimalLatitude = latitude,      decimalLongitude = longitude,     geodeticDatum = \"WGS84\",     coordinateUncertaintyInMeters = 30     ) |>   relocate(decimalLatitude, decimalLongitude, .after = eventID) # re-position cols #> ⠙ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #> ⠹ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #> ✔ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #>   obs_id_site #> # A tibble: 123 × 15 #>    eventID   decimalLatitude decimalLongitude site_code  year  cpar  csig limdum #>    <chr>               <dbl>            <dbl> <chr>     <dbl> <dbl> <dbl>  <dbl> #>  1 0001-AMA…           -35.2             149. AMA100     2004     1     0      0 #>  2 0002-AMA…           -35.2             149. AMA100     2007     1     0      1 #>  3 0003-AMA…           -35.2             149. AMA100     2007     1     0      1 #>  4 0004-AMA…           -35.2             149. AMA100     2005     1     1      1 #>  5 0005-AMA…           -35.2             149. AMA100     2008     1     0      1 #>  6 0006-AMA…           -35.2             149. AMA100     2008     1     0      1 #>  7 0007-AMA…           -35.2             149. AMA100     2013     1     0      1 #>  8 0008-AMA…           -35.2             149. AMA100     2008     1     0      1 #>  9 0009-AMA…           -35.2             149. AMA100     2013     1     1      0 #> 10 0010-AMA…           -35.2             149. AMA100     2014     1     1      1 #> # ℹ 113 more rows #> # ℹ 7 more variables: limper <dbl>, limtas <dbl>, lper <dbl>, lver <dbl>, #> #   ulae <dbl>, coordinateUncertaintyInMeters <dbl>, geodeticDatum <chr> events <- obs_id_site |>   select(     any_of(event_terms())     )  events #> # A tibble: 123 × 6 #>    eventID           year decimalLatitude decimalLongitude geodeticDatum #>    <chr>            <dbl>           <dbl>            <dbl> <chr>         #>  1 0001-AMA100-2004  2004           -35.2             149. WGS84         #>  2 0002-AMA100-2007  2007           -35.2             149. WGS84         #>  3 0003-AMA100-2007  2007           -35.2             149. WGS84         #>  4 0004-AMA100-2005  2005           -35.2             149. WGS84         #>  5 0005-AMA100-2008  2008           -35.2             149. WGS84         #>  6 0006-AMA100-2008  2008           -35.2             149. WGS84         #>  7 0007-AMA100-2013  2013           -35.2             149. WGS84         #>  8 0008-AMA100-2008  2008           -35.2             149. WGS84         #>  9 0009-AMA100-2013  2013           -35.2             149. WGS84         #> 10 0010-AMA100-2014  2014           -35.2             149. WGS84         #> # ℹ 113 more rows #> # ℹ 1 more variable: coordinateUncertaintyInMeters <dbl> events |> use_data()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"prepare-occurrences-csv","dir":"Articles","previous_headings":"","what":"Prepare occurrences.csv","title":"Standardise an Event dataset","text":"Let’s return obs_id_site, contains eventID site information sample. create Occurrence-based dataframe conforms Darwin Core Standard, need transpose wide-format dataframe long format, row contains one observation. ’ll select eventID abbreviated species columns, pivot data species observation abbreviation presence/absence recorded presence. Now ’ll merge correct names frog species joining species obs_long. Now can reformat data use valid Darwin Core column names using set_ functions. Importantly, Darwin Core Standard requires add unique occurrenceID type observation column basisOfRecord. now dataframe observations organised occurrence-level. final step reduce obs_long_dwc include columns valid column names Occurrence-based datasets. drops abbreviation column dataframe. can specify wish use occurrences Darwin Core Archive use_data(), save occurrences csv file default directory data-publish ./data-publish/occurrences.csv. data terms, ’s ! Don’t forget add metadata using use_metadata_template() use_metadata() build submit archive.","code":"obs_long <- obs_id_site |>   select(eventID, any_of(species$abbreviation)) |>   pivot_longer(cols = species$abbreviation,                names_to = \"abbreviation\",                values_to = \"presence\")  obs_long #> # A tibble: 984 × 3 #>    eventID          abbreviation presence #>    <chr>            <chr>           <dbl> #>  1 0001-AMA100-2004 cpar                1 #>  2 0001-AMA100-2004 csig                0 #>  3 0001-AMA100-2004 limdum              0 #>  4 0001-AMA100-2004 limper              0 #>  5 0001-AMA100-2004 limtas              1 #>  6 0001-AMA100-2004 lper                1 #>  7 0001-AMA100-2004 lver                0 #>  8 0001-AMA100-2004 ulae                0 #>  9 0002-AMA100-2007 cpar                1 #> 10 0002-AMA100-2007 csig                0 #> # ℹ 974 more rows obs_long <- obs_long |>   left_join(species, join_by(abbreviation), keep = FALSE) |>   relocate(presence, .after = last_col()) # re-position column obs_long_dwc <- obs_long |>  set_occurrences(    occurrenceID = composite_id(eventID, sequential_id()),    basisOfRecord = \"humanObservation\",    occurrenceStatus = dplyr::case_when(presence == 1 ~ \"present\",                                        .default = \"absent\")    ) |>  set_scientific_name(    scientificName = scientific_name    ) |>  set_taxonomy(    vernacularName = common_name    ) #> ⠙ Checking 3 columns: occurrenceID, basisOfRecord, and occurrenceStatus #> ✔ Checking 3 columns: occurrenceID, basisOfRecord, and occurrenceStatus [919ms] #>  #> ⠙ Checking 1 column: scientificName #> ✔ Checking 1 column: scientificName [311ms] #>  #> ⠙ Checking 1 column: vernacularName #> ✔ Checking 1 column: vernacularName [311ms] #>   obs_long_dwc #> # A tibble: 984 × 7 #>    eventID          abbreviation occurrenceID     basisOfRecord occurrenceStatus #>    <chr>            <chr>        <chr>            <chr>         <chr>            #>  1 0001-AMA100-2004 cpar         0001-AMA100-200… humanObserva… present          #>  2 0001-AMA100-2004 csig         0001-AMA100-200… humanObserva… absent           #>  3 0001-AMA100-2004 limdum       0001-AMA100-200… humanObserva… absent           #>  4 0001-AMA100-2004 limper       0001-AMA100-200… humanObserva… absent           #>  5 0001-AMA100-2004 limtas       0001-AMA100-200… humanObserva… present          #>  6 0001-AMA100-2004 lper         0001-AMA100-200… humanObserva… present          #>  7 0001-AMA100-2004 lver         0001-AMA100-200… humanObserva… absent           #>  8 0001-AMA100-2004 ulae         0001-AMA100-200… humanObserva… absent           #>  9 0002-AMA100-2007 cpar         0002-AMA100-200… humanObserva… present          #> 10 0002-AMA100-2007 csig         0002-AMA100-200… humanObserva… absent           #> # ℹ 974 more rows #> # ℹ 2 more variables: scientificName <chr>, vernacularName <chr> occurrences <- obs_long_dwc |>   select(     any_of(occurrence_terms())     ) occurrences |> use_data()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Standardise an Event dataset","text":"hierarchical structure Event-based data (ie Site -> Sample -> Occurrence) adds richness, allowing information like repeated sampling presence/absence information preserved. richness can enable nuanced probabilistic analyses like species distribution models occupancy models. encourage users Event-based data use galaxias standardise data publication sharing.","code":""},{"path":"https://galaxias.ala.org.au/R/articles/occurrences-example.html","id":"the-dataset","dir":"Articles","previous_headings":"","what":"The dataset","title":"Standardise an Occurrence dataset","text":"Let’s use small example dataset bird observations taken 4 different site locations. dataset many different types data like landscape type age class. Importantly standardising Darwin Core, dataset contains scientific name (species), coordinate location (lat & lon) date observation (date).","code":"library(galaxias) library(dplyr) library(readr)  obs <- read_csv(\"dummy-dataset-sb.csv\",                 show_col_types = FALSE) |>   janitor::clean_names()  obs |>    gt::gt() |>   gt::opt_interactive(page_size_default = 5)"},{"path":"https://galaxias.ala.org.au/R/articles/occurrences-example.html","id":"standardise-to-darwin-core","dir":"Articles","previous_headings":"","what":"Standardise to Darwin Core","title":"Standardise an Occurrence dataset","text":"determine need standardise dataset, let’s use suggest_workflow(). output tells us one matching Darwin Core term data already (sex), missing minimum required Darwin Core terms. “Suggest workflow”, output suggests series piped set_ functions can use rename, modify add columns missing obs required Darwin Core. set_ functions specialised wrappers around dplyr::mutate(), additional functionality support using Darwin Core Standard. simplicity, let’s easy part first renaming columns already dataset use accepted standard Darwin Core terms. set_ functions automatically check make sure column correctly formatted. ’ll save modified dataframe obs_dwc. Running suggest_workflow() reflect progress show us ’s left . Now output tells us still need add several columns dataset meet minimum Darwin Core requirements. ’s rundown columns need add: occurrenceID: Unique identifiers record. ensures can identify specific record future updates corrections. can use composite_id(), sequential_id() random_id() add unique IDs row. basisOfRecord: type record (e.g. human observation, specimen, machine observation). See list acceptable values corella::basisOfRecord_values(). geodeticDatum: Coordinate Reference System (CRS) projection data (example, CRS Google Maps “WGS84”). coordinateUncertaintyInMeters: area uncertainty around observation. may know value based method data collection Now let’s add columns using set_occurrences() set_coordinates(). can also add suggested function set_individual_traits() automatically identify matched column name sex check column’s format. Running suggest_workflow() confirm dataset ready used Darwin Core Archive! submit dataset, let’s select columns valid occurrence term names save dataframe file occurrences.csv. Importantly, save csv folder called data-processed, galaxias looks automatically building Darwin Core Archive. final step save ‘publishing’ directory: done! See Quick start guide vignette build Darwin Core Archive.","code":"obs |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 1 of 12 column names to DwC terms: #> ✔ Matched: sex #> ✖ Unmatched: age_class, comments, date, landscape, lat, lon, molecular_sex, #>   sample_id, site, species, species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)  Missing term(s)                                                                 #> ✖ Identifier (at least one) -                occurrenceID, catalogNumber, recordNumber                                        #> ✖ Record type               -                basisOfRecord                                                                    #> ✖ Scientific name           -                scientificName                                                                   #> ✖ Location                  -                decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters  #> ✖ Date/Time                 -                eventDate #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_datetime() |> #>   set_coordinates() |> #>   set_scientific_name() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs |>   set_scientific_name(scientificName = species) |>   set_coordinates(decimalLatitude = lat,                   decimalLongitude = lon) |>   set_datetime(eventDate = lubridate::ymd(date)) # specify year-month-day format #> ⠙ Checking 1 column: scientificName #> ⠹ Checking 1 column: scientificName #> ✔ Checking 1 column: scientificName [327ms] #>  #> ⠙ Checking 2 columns: decimalLatitude and decimalLongitude #> ✔ Checking 2 columns: decimalLatitude and decimalLongitude [623ms] #>  #> ⠙ Checking 1 column: eventDate #> ✔ Checking 1 column: eventDate [311ms] #> obs_dwc |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 5 of 12 column names to DwC terms: #> ✔ Matched: decimalLatitude, decimalLongitude, eventDate, scientificName, sex #> ✖ Unmatched: age_class, comments, landscape, molecular_sex, sample_id, site, #>   species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)                  Missing term(s)                             #> ✔ Scientific name           scientificName                   -                                            #> ✔ Date/Time                 eventDate                        -                                            #> ✖ Identifier (at least one) -                                occurrenceID, catalogNumber, recordNumber    #> ✖ Record type               -                                basisOfRecord                                #> ✖ Location                  decimalLatitude decimalLongitude geodeticDatum coordinateUncertaintyInMeters #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_coordinates() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs_dwc |>   set_occurrences(     occurrenceID = composite_id(sequential_id(), site, landscape),     basisOfRecord = \"humanObservation\"     ) |>   set_coordinates(     geodeticDatum = \"WGS84\",     coordinateUncertaintyInMeters = 30     # coordinateUncertaintyInMeters = with_uncertainty(method = \"phone\")     ) |>   set_individual_traits() #> ⠙ Checking 2 columns: occurrenceID and basisOfRecord #> ⠹ Checking 2 columns: occurrenceID and basisOfRecord #> ✔ Checking 2 columns: occurrenceID and basisOfRecord [631ms] #>  #> ⠙ Checking 4 columns: decimalLatitude, decimalLongitude, coordinateUncertaintyI… #> ✔ Checking 4 columns: decimalLatitude, decimalLongitude, coordinateUncertaintyI… #>  #> ⠙ Checking 1 column: sex #> ✔ Checking 1 column: sex [311ms] #> obs_dwc |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 9 of 16 column names to DwC terms: #> ✔ Matched: basisOfRecord, coordinateUncertaintyInMeters, decimalLatitude, #>   decimalLongitude, eventDate, geodeticDatum, occurrenceID, scientificName, sex #> ✖ Unmatched: age_class, comments, landscape, molecular_sex, sample_id, site, #>   species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)                                                                 Missing term(s)  #> ✔ Identifier (at least one) occurrenceID                                                                    -                 #> ✔ Record type               basisOfRecord                                                                   -                 #> ✔ Scientific name           scientificName                                                                  -                 #> ✔ Location                  decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters -                 #> ✔ Date/Time                 eventDate                                                                       -                 #>  #>  #> 🥇 All minimum column requirements met! #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> 🥇 Your dataframe is Darwin Core compliant! #> Run checks, or use your dataframe to build a Darwin Core Archive with galaxias: #> df |> #>   check_dataset() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs_dwc |>   select(any_of(occurrence_terms())) # select any matching terms  obs_dwc |>   gt::gt() |>   gt::opt_interactive(page_size_default = 5) # Save in ./data-processed use_data_occurrences(obs_dwc)"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"Quick start guide","text":"existing R project containing data collected course research project. project uses fairly standard folder structure. Let’s see galaxias can help us package data Darwin Core Archive.","code":"├── README.md                        : Description of the repository ├── my-project-name.Rproj            : RStudio project file ├── data                             : Folder to store cleaned data |  └── my_data.csv ├── data-raw                         : Folder to store original/source data |  └── my_raw_data.csv ├── plots                            : Folder containing plots/dataviz └── scripts                          : Folder with analytic coding scripts"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"use-standardised-data-in-an-archive","dir":"Articles","previous_headings":"","what":"Use standardised data in an archive","title":"Quick start guide","text":"Data wish share data folder. might look something like : First, ’ll need standardise data conform Darwin Core Standard. suggest_workflow() can help summarising dataset suggesting steps take. Following advice suggest_workflow(), can use set_ functions standardise my_data. set_ functions work lot like dplyr::mutate(): modify existing columns create new columns. suffix set_ function gives indication type data accepts (e.g. set_coordinates(), set_scientific_name), function arguments valid Darwin Core terms use column names. set_ function also checks make sure column contains valid data according Darwin Core Standard. may noticed added additional columns included advice suggest_workflow() (country, locality, taxonRank, kingdom, family). encourage users specify additional information possible avoid ambiguity data shared. use standardised data Darwin Core Archive, can select columns use valid Darwin Core terms column names. Invalid columns won’t accepted try build Darwin Core Archive. data occurrence-based dataset (row contains information observation level, opposed site/survey level), ’ll select columns match names occurrence_terms(). Now can specify wish use my_data_dwc_occ Darwin Core Archive use_data(), saves dataset data_publish folder correct file name occurrences.csv. look file structure, now find data added new folder:","code":"my_data #> # A tibble: 2 × 6 #>   latitude longitude date       time  species                  location_id #>      <dbl>     <dbl> <chr>      <chr> <chr>                    <chr>       #> 1    -35.3      149. 14-01-2023 10:23 Callocephalon fimbriatum ARD001      #> 2    -35.3      149. 15-01-2023 11:25 Eolophus roseicapilla    ARD001 my_data |> suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 0 of 6 column names to DwC terms: #> ✔ Matched: #> ✖ Unmatched: date, latitude, location_id, longitude, species, time #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)  Missing term(s)                                                                 #> ✖ Identifier (at least one) -                occurrenceID, catalogNumber, recordNumber                                        #> ✖ Record type               -                basisOfRecord                                                                    #> ✖ Scientific name           -                scientificName                                                                   #> ✖ Location                  -                decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters  #> ✖ Date/Time                 -                eventDate #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_datetime() |> #>   set_coordinates() |> #>   set_scientific_name() #>  #> ── Additional functions #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms library(lubridate)  my_data_dwc <- my_data |>   # basic requirements of Darwin Core   set_occurrences(occurrenceID = sequential_id(),                   basisOfRecord = \"humanObservation\") |>    # place and time   set_coordinates(decimalLatitude = latitude,                    decimalLongitude = longitude) |>   set_locality(country = \"Australia\",                 locality = \"Canberra\") |>   set_datetime(eventDate = lubridate::dmy(date),                eventTime = lubridate::hm(time)) |>   # taxonomy   set_scientific_name(scientificName = species,                        taxonRank = \"species\") |>   set_taxonomy(kingdom = \"Animalia\",                family = \"Cacatuidae\")   my_data_dwc  #> # A tibble: 2 × 13 #>   location_id basisOfRecord    occurrenceID decimalLatitude decimalLongitude #>   <chr>       <chr>            <chr>                  <dbl>            <dbl> #> 1 ARD001      humanObservation 01                     -35.3             149. #> 2 ARD001      humanObservation 02                     -35.3             149. #> # ℹ 8 more variables: country <chr>, locality <chr>, eventDate <date>, #> #   eventTime <Period>, scientificName <chr>, taxonRank <chr>, family <chr>, #> #   kingdom <chr> library(dplyr)  my_data_dwc_occ <- my_data_dwc |>   select(any_of(occurrence_terms()))  my_data_dwc_occ ## # A tibble: 2 × 12 ##   basisOfRecord    occurrenceID eventDate  eventTime  country   locality ##   <chr>            <chr>        <date>     <Period>   <chr>     <chr>    ## 1 humanObservation 01           2023-01-14 10H 23M 0S Australia Canberra ## 2 humanObservation 02           2023-01-15 11H 25M 0S Australia Canberra ## # ℹ 6 more variables: decimalLatitude <dbl>, decimalLongitude <dbl>, ## #   scientificName <chr>, kingdom <chr>, family <chr>, taxonRank <chr> use_data(my_data_dwc_occ) ├── README.md ├── my-project-name.Rproj ├── data |  └── my_data.csv ├── data-publish                    : New folder to store data for publication |  └── occurrences.csv              : Data formatted as per Darwin Core Standard ├── data-raw |  └── my_raw_data.csv ├── plots └── scripts"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"add-metadata","dir":"Articles","previous_headings":"","what":"Add metadata","title":"Quick start guide","text":"critical part Darwin Core archive metadata statement: tells users owns data, data collected , uses can put (.e. data licence). get example statement, call use_metadata_template(). default, creates R Markdown template named metadata.Rmd working directory. can edit template include information dataset, specify wish use Darwin Core Archive use_metadata(). converts metadata statement Ecological Meta Language (EML), accepted format metadata Darwin Core Archives, saves eml.xml data-publish folder.","code":"use_metadata_template() use_metadata(\"metadata.Rmd\")"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"build-an-archive","dir":"Articles","previous_headings":"","what":"Build an archive","title":"Quick start guide","text":"end process, folder named data-publish contains least two files: One .csv files containing data (e.g. occurrences.csv, events.csv, multimedia.csv) eml.xml file containing metadata can now run build_archive() build Darwin Core Archive! Running build_archive() first checks whether ‘schema’ document (meta.xml) data-publish folder. machine-readable xml document describes content archive’s data files structure. schema document required file Darwin Core Archive. missing, build_archive() build one. can also build schema document using use_schema(). end process, Darwin Core Archive zip file (dwc-archive.zip) paernt directory. also data-publish folder working directory containing standardised data files (e.g. occurrences.csv), metadata statement EML format (eml.xml), schema document (meta.xml).","code":"build_archive()"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"check-archive","dir":"Articles","previous_headings":"","what":"Check archive","title":"Quick start guide","text":"two ways check whether contents Darwin Core Archive meet Darwin Core Standard. first run local tests files inside local folder directory used build Darwin Core Archive. check_directory() allows us check csv files xml files directory Darwin Core Standard criteria, using checking functionality built set_ functions. function especially beneficial standardized data Darwin Core headers using functions outside galaxias/corella, dplyr::mutate() example. second check whether complete Darwin Core Archive meets institution’s Darwin Core criteria via API. example, can test archive GBIF’s API tests.","code":"check_directory() # Check against GBIF API check_archive(\"dwc-archive.zip\",               email = \"your-email\",               username = \"your-username\",               password = \"your-password\")"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"publishshare-your-archive","dir":"Articles","previous_headings":"","what":"Publish/share your archive","title":"Quick start guide","text":"final step share completed Darwin Core Archive data infrastructure like Atlas Living Australia. share ALA, can launch data submission process browser calling: function provide option open GitHub issue can attach archive. run galaxias test suite dataset respond soon can. ’d prefer use GitHub, can send file brief description support@ala.org.au.","code":"submit_archive()"},{"path":"https://galaxias.ala.org.au/R/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martin Westgate. Author, maintainer. Shandiya Balasubramaniam. Author. Dax Kellie. Author.","code":""},{"path":"https://galaxias.ala.org.au/R/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Westgate M, Balasubramaniam S, Kellie D (2025). galaxias: Describe, Package, Share Biodiversity Data. R package version 0.1.1, https://galaxias.ala.org.au.","code":"@Manual{,   title = {galaxias: Describe, Package, and Share Biodiversity Data},   author = {Martin Westgate and Shandiya Balasubramaniam and Dax Kellie},   year = {2025},   note = {R package version 0.1.1},   url = {https://galaxias.ala.org.au}, }"},{"path":[]},{"path":"https://galaxias.ala.org.au/R/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Describe, Package, and Share Biodiversity Data","text":"galaxias R package helps users describe, bundle, share biodiversity information using ‘Darwin Core’ data standard. galaxias provides tools R build Darwin Core Archive, zip file containing standardised data metadata accepted global data infrastructures. package mirrors functionality devtools, usethis, dplyr manage data, files, folders. galaxias created Science & Decision Support Team Atlas Living Australia (ALA). package named genus freshwater fish found Southern Hemisphere, predominantly Australia Aotearoa New Zealand. logo shows Spotted Galaxias (Galaxias truttaceus) drawn Ian Brennan. comments, questions, suggestions, please contact us.","code":""},{"path":"https://galaxias.ala.org.au/R/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Describe, Package, and Share Biodiversity Data","text":"can install latest version GitHub : CRAN, can use: load package, call:","code":"install.packages(\"remotes\") remotes::install_github(\"atlasoflivingaustralia/galaxias\") install.packages(\"galaxias\") library(galaxias)"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Describe, Package, and Share Biodiversity Data","text":"galaxias contains tools : Standardise tibbles containing biodiversity observations match Darwin Core Standard. Convert metadata statements written R Markdown Quarto EML files. Store publication-ready files single directory, zip directory publication. Check files consistency Darwin Core Standard, either locally using via API. galaxias draws functionality two underlying packages address different challenges data publication workflow: corella, converts tibbles use standard column names; delma converts markdown files EML format.","code":""},{"path":"https://galaxias.ala.org.au/R/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Describe, Package, and Share Biodiversity Data","text":"small example dataset species observations. can standardise data according Darwin Core Standard using set_ functions. can specify wish use standardised data Darwin Core Archive use_data(). saves df_dwc valid file name extension, standardised location (new directory called /data-publish). publishing data, also necessary create metadata statement describes owns data, data shows, licence released . galaxias enables write metadata statement R Markdown Quarto format, seamlessly convert EML publication. final step data publication workflow zip directory single file. file placed parent directory. can share data via mechanism wish, galaxias provides submit_archive() function open submission window Atlas Living Australia. Please see Quick Start Guide -depth explanation building Darwin Core Archives.","code":"library(tibble)  df <- tibble(   scientificName = c(\"Callocephalon fimbriatum\", \"Eolophus roseicapilla\"),   latitude = c(-35.310, -35.273),    longitude = c(149.125, 149.133),   eventDate = lubridate::dmy(c(\"14-01-2023\", \"15-01-2023\")),   status = c(\"present\", \"present\") )  df #> # A tibble: 2 × 5 #>   scientificName           latitude longitude eventDate  status  #>   <chr>                       <dbl>     <dbl> <date>     <chr>   #> 1 Callocephalon fimbriatum    -35.3      149. 2023-01-14 present #> 2 Eolophus roseicapilla       -35.3      149. 2023-01-15 present df_dwc <- df |>    set_occurrences(occurrenceID = random_id(),                    basisOfRecord = \"humanObservation\",                    occurrenceStatus = status) |>    set_coordinates(decimalLatitude = latitude,                    decimalLongitude = longitude)  df_dwc #> # A tibble: 2 × 7 #>   scientificName          eventDate  basisOfRecord occurrenceID occurrenceStatus #>   <chr>                   <date>     <chr>         <chr>        <chr>            #> 1 Callocephalon fimbriat… 2023-01-14 humanObserva… e16986de-57… present          #> 2 Eolophus roseicapilla   2023-01-15 humanObserva… e16986f2-57… present          #> # ℹ 2 more variables: decimalLatitude <dbl>, decimalLongitude <dbl> use_data(df_dwc) # 1. Create a boilerplate file use_metadata_template(\"metadata.Rmd\")  # 2. Edit in your preferred IDE  # 3. Load into /data-publish as an EML file use_metadata(\"metadata.Rmd\") build_archive(file = \"my_biodiversity_data.zip\")"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"citing-galaxias","dir":"","previous_headings":"","what":"Citing galaxias","title":"Describe, Package, and Share Biodiversity Data","text":"generate citation package version using, can run: current recommended citation : Westgate MJ, Balasubramaniam S & Kellie D (2025) galaxias: Describe, Package, Share Biodiversity Data. R Package version 0.1.0.","code":"citation(package = \"galaxias\")"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"contributors","dir":"","previous_headings":"","what":"Contributors","title":"Describe, Package, and Share Biodiversity Data","text":"Developers contributed galaxias follows (alphabetical order surname): Amanda Buyan (@acbuyan), Fonti Kar (@fontikar), Peggy Newman (@peggynewman) & Andrew Schwenke (@andrew-1234)","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a Darwin Core Archive from a folder — build_archive","title":"Build a Darwin Core Archive from a folder — build_archive","text":"Darwin Core archive zip file containing combination data metadata. build_archive() constructs zip file parent directory. function assumes necessary files pre-constructed, can found inside \"data-publish\" directory additional redundant information. Structurally, build_archive() similar devtools::build(), sense takes repository wraps publication.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a Darwin Core Archive from a folder — build_archive","text":"","code":"build_archive(file = \"dwc-archive.zip\", overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a Darwin Core Archive from a folder — build_archive","text":"file name file built parent directory. end .zip. overwrite (logical) existing files overwritten? Defaults FALSE. quiet (logical) Whether suppress messages happening. Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a Darwin Core Archive from a folder — build_archive","text":"return anything; called side-effect building 'Darwin Core Archive' (.e. zip file).","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a Darwin Core Archive from a folder — build_archive","text":"function looks three types objects data-publish directory: Data One csv files named occurrences.csv, events.csv /multimedia.csv. csv files contain data standardised using Darwin Core Standard (see corella::corella-package() details). data.frame/tibble can added correct folder using use_data(). Metadata metadata statement EML format file name eml.xml. Completed metadata statements written markdown .Rmd qmd files can converted saved correct folder using use_metadata(). Create new template use_metadata_template(). Schema 'schema' document xml format file name meta.xml. build_archive() detect whether file present build schema file missing. file can also constructed separately using use_schema().","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether an archive meets the Darwin Core Standard via API — check_archive","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"Check whether specified Darwin Core Archive ready sharing publication, according Darwin Core Standard. check_archive() tests archive - defaulting \"dwc-archive.zip\" users' parent directory - using online validation service. Currently supports validation using GBIF.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"","code":"check_archive(   file = \"dwc-archive.zip\",   username = NULL,   email = NULL,   password = NULL,   wait = TRUE,   quiet = FALSE )  get_report(   obj,   username = NULL,   password = NULL,   n = 5,   wait = TRUE,   quiet = FALSE )  view_report(x, n = 5)  # S3 method for class 'gbif_validator' print(x, ...)"},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"file name file parent directory pass validator API, ideally created using build_archive(). username GBIF username. email email address used register gbif.org. password GBIF password. wait (logical) Whether wait completed report API exiting (TRUE, default), try API return result regardless (FALSE). quiet (logical) Whether suppress messages happening. Default set FALSE; .e. messages shown. obj Either object class character containing key uniquely identifies query; object class gbif_validator. returned check_archive() get_report() n Maximum number entries print per file. Defaults 5. x object class gbif_validator. ... Additional arguments, currently ignored.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"check_archive() get_report() return object class gbif_validator workspace. view_report() print.gbif_validator() return anything, called side-effect printing useful information console.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"Internally, check_archive() POSTs specified archive GBIF validator API calls get_report() retrieve (GET) result. get_report() exported allow user download results later time wish; efficient repeatedly generating queries check_archive() underlying data unchanged. third option simply assign outcome check_archive() get_report() object, call view_report() format result nicely. approach require API calls considerably faster. Note information returned functions provided verbatim institution API, galaxias.","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"Checks files data-publish directory meet Darwin Core Standard. check_directory() runs corella::check_dataset() occurrences.csv events.csv files, delma::check_metadata() eml.xml file, present. check_ functions run tests determine whether data metadata pass Darwin Core Standard criteria.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"","code":"check_directory()"},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"return anything; called side-effect generating report console.","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Build repositories to share biodiversity data — galaxias-package","title":"Build repositories to share biodiversity data — galaxias-package","text":"galaxias helps users describe, package share biodiversity information using 'Darwin Core' data standard, format used accepted Global Biodiversity Information Facility (GBIF) ' partner nodes. galaxias functionally similar devtools, focus building Darwin Core Archives rather R packages. package named genus freshwater fish.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Build repositories to share biodiversity data — galaxias-package","text":"questions, comments suggestions, please email support@ala.org.au. Prepare information Darwin Core use_metadata_template() Add blank metadata statement template working directory suggest_workflow() Advice standardise data using Darwin Core Standard Add information data-publish directory use_data() Save standardised data use Darwin Core Archive use_metadata() Convert metadata file markdown EML (eml.xml) save use Darwin Core Archive use_schema() Build schema file (meta.xml) given directory save use Darwin Core Archive Build archive check_directory() Check files local Darwin Core directory build_archive() Convert directory Darwin Core Archive check_archive() Check whether archive passes Darwin Core criteria via GBIF API submit_archive() Open browser submit data ALA","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Build repositories to share biodiversity data — galaxias-package","text":"Maintainer: Martin Westgate martin.westgate@csiro.au Authors: Shandiya Balasubramaniam shandiya.balasubramaniam@csiro.au Dax Kellie dax.kellie@csiro.au","code":""},{"path":"https://galaxias.ala.org.au/R/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. corella suggest_workflow delma use_metadata_template","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit a Darwin Core Archive to the ALA — submit_archive","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"preferred method submitting dataset publication via ALA raise issue 'Data Publication' GitHub Repository, attached archive zip file (constructed using build_archive()) issue. dataset especially large (>100MB), need post publicly accessible location (GitHub release) post link instead. function simply opens new issue users' default browser enable dataset submission.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"","code":"submit_archive(quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"quiet Whether suppress messages happening. Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"return anything workspace; called side-effect opening submission form users' default browser.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"process accepting data publication ALA automated; function initiate evaluation process, result data instantly visible ALA. submission guarantee acceptance, ALA reserves right refuse publish data reveals locations threatened -risk species. mechanism entirely public; data visible others point put webpage. data contains sensitive information, contact support@ala.org.au arrange different delivery mechanism.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"","code":"if(interactive()){   submit_archive() }"},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Use standardised data in a Darwin Core Archive — use_data","title":"Use standardised data in a Darwin Core Archive — use_data","text":"data conform Darwin Core Standard, use_data() makes easy save data correct place building Darwin Core Archive build_archive(). use_data() --one function accepted data types \"occurrence\", \"event\" \"multimedia\". use_data() attempts detect save correct data type based provided tibble/data.frame. Alternatively, users can call underlying functions use_data_occurrences() use_data_events() specify data type manually.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use standardised data in a Darwin Core Archive — use_data","text":"","code":"use_data(..., overwrite = FALSE, quiet = FALSE)  use_data_occurrences(df, overwrite = FALSE, quiet = FALSE)  use_data_events(df, overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use standardised data in a Darwin Core Archive — use_data","text":"... Unquoted name tibble/data.frame save. overwrite default, use_data_events() overwrite existing files. really want , set TRUE. quiet Whether message happening. Default set FALSE. df tibble/data.frame save.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use standardised data in a Darwin Core Archive — use_data","text":"return anything workspace; called side-effect saving .csv file /data-publish.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use standardised data in a Darwin Core Archive — use_data","text":"function saves data data-publish folder. create folder already present. Data type determined detecting type-specific column names supplied data. Event: (eventID, parentEventID, eventType) Multimedia: yet supported","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use standardised data in a Darwin Core Archive — use_data","text":"","code":"#> ✔ Setting active project to \"/tmp/RtmpvRvQe5\".  # Build an example dataset df <- tibble::tibble(   occurrenceID = c(\"a1\", \"a2\"),   species = c(\"Eolophus roseicapilla\", \"Galaxias truttaceus\"))  # The default function *always* asks about data type if(interactive()){   use_data(df) }  # To manually specify the type of data - and avoid questions in your  # console - use the underlying functions instead use_data_occurrences(df, quiet = TRUE) #> ✔ Creating data-publish/.  # Check that file has been created list.files(\"data-publish\") #> [1] \"occurrences.csv\"  # returns \"occurrences.csv\" as expected  #> ✔ Setting active project to #>   \"/home/runner/work/galaxias/galaxias/docs/reference\"."},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Use a metadata statement in a Darwin Core Archive — use_metadata","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"metadata statement lists owner dataset, collected, can used (.e. ' licence). function reads converts metadata saved markdown (.md), Rmarkdown (.Rmd) Quarto (.qmd) xml, saves data-publish directory. function convenience wrapper function delma::read_md() delma::write_eml().","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"","code":"use_metadata(file = NULL, overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"file metadata file Rmarkdown (.Rmd) Quarto markdown (.qmd) format. overwrite default, use_metadata() overwrite existing files. really want , set TRUE. quiet Whether message happening. Default set FALSE.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"return object workspace; called side effect building file data-publish directory.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"compliant Darwin Core Standard, schema file must called eml.xml, function enforces .","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"","code":"#> ✔ Setting active project to \"/tmp/RtmpvRvQe5\".  # Get a boilerplate metadata statement use_metadata_template(file = \"my_metadata.Rmd\", quiet = TRUE)  # Once editing is complete, call `use_metadata()` to convert to an EML file use_metadata(\"my_metadata.Rmd\", quiet = TRUE) #> ✔ Creating data-publish/.  # Check that file has been created list.files(\"data-publish\") #> [1] \"eml.xml\"  # returns \"eml.xml\" as expected  #> ✔ Setting active project to #>   \"/home/runner/work/galaxias/galaxias/docs/reference\"."},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a schema for a Darwin Core Archive — use_schema","title":"Create a schema for a Darwin Core Archive — use_schema","text":"schema xml document maps files field names DwCA. map makes easier reconstruct one related datasets information matched correctly. works detecting column names csv files specified directory; Darwin Core terms function produce reliable results. function assumes publishing directory named \"data-publish\". function primarily internal called build_archive(), exported clarity debugging purposes.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a schema for a Darwin Core Archive — use_schema","text":"","code":"use_schema(overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a schema for a Darwin Core Archive — use_schema","text":"overwrite default, use_schema() overwrite existing files. really want , set TRUE. quiet (logical) progress messages suppressed? Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a schema for a Darwin Core Archive — use_schema","text":"return object workspace; called side effect building schema file publication directory.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a schema for a Darwin Core Archive — use_schema","text":"compliant Darwin Core Standard, schema file must called meta.xml, function enforces .","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a schema for a Darwin Core Archive — use_schema","text":"","code":"#> ✔ Setting active project to \"/tmp/RtmpvRvQe5\".  # First build some data to add to our archive df <- tibble::tibble(   occurrenceID = c(\"a1\", \"a2\"),   species = c(\"Eolophus roseicapilla\", \"Galaxias truttaceus\"))    use_data_occurrences(df, quiet = TRUE) #> ✔ Creating data-publish/.  # Now we can build a schema document to describe that dataset use_schema(quiet = TRUE)  # Check that specified files have been created list.files(\"data-publish\")  #> [1] \"meta.xml\"        \"occurrences.csv\"  # The publish directory now contains: #  - \"occurrences.csv\" which contains data #  - \"meta.xml\" which is the schema document  #> ✔ Setting active project to #>   \"/home/runner/work/galaxias/galaxias/docs/reference\"."},{"path":[]},{"path":"https://galaxias.ala.org.au/R/news/index.html","id":"bug-fixes-0-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"galaxias 0.1.1","text":"Fix bug meta.xml included final zip file built via build_archive() workflow (#57) use_data() accepts data.frame (#56)","code":""},{"path":"https://galaxias.ala.org.au/R/news/index.html","id":"galaxias-010","dir":"Changelog","previous_headings":"","what":"galaxias 0.1.0","title":"galaxias 0.1.0","text":"CRAN release: 2025-07-07 First release version. hope like !","code":""}]
