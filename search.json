[{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"the-dataset","dir":"Articles","previous_headings":"","what":"The dataset","title":"Standardise an Events dataset","text":"example, ’ll use dataset containing observations frogs [x area x time]. Data collected using 5-minute audio surveys, row documents whether frog species detected 5-minute recording, recorded present (1) absent (0). dataset consists 3 spreadsheets: species list, observations sites.","code":""},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"species-list","dir":"Articles","previous_headings":"","what":"Standardise an Events dataset","title":"Standardise an Events dataset","text":"species list spreadsheet lists eight frog species recorded dataset. abbreviation column contains abbreviated column name used observations spreadsheet.","code":"library(readxl) library(dplyr) library(tidyr)  species <- read_xlsx(\"Frogwatch_dataset.xlsx\",                       sheet = \"species list\") |>    janitor::clean_names()  species #> # A tibble: 8 × 3 #>   scientific_name            common_name            abbreviation #>   <chr>                      <chr>                  <chr>        #> 1 Crinia parinsignifera      Plains Froglet         cpar         #> 2 Crinia signifera           Common Eastern Froglet csig         #> 3 Limnodynastes dumerilii    Pobblebonk             limdum       #> 4 Limnodynastes peronii      Striped Marsh Frog     limper       #> 5 Limnodynastes tasmaniensis Spotted Grass Frog     limtas       #> 6 Litoria peronii            Emerald Spotted Frog   lper         #> 7 Litoria verreauxii         Alpine Tree Frog       lver         #> 8 Uperoleia laevigata        Smooth Toadlet         ulae"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"observations","dir":"Articles","previous_headings":"","what":"Standardise an Events dataset","title":"Standardise an Events dataset","text":"observations spreadsheet contains columns describe sample’s location (e.g. site_code, water_type, veg_canopy) occurrence frog species (e.g. cpar, csig, limdum).","code":"obs <- read_xlsx(\"Frogwatch_dataset.xlsx\",                   sheet = \"observations\") |>   janitor::clean_names()  obs |> rmarkdown::paged_table()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"sites","dir":"Articles","previous_headings":"","what":"Standardise an Events dataset","title":"Standardise an Events dataset","text":"sites spreadsheet contains columns describe site location (e.g. depth, water_type, latitude, longitude) overall presence/absence frog species site (e.g. cpar, csig, limdum).","code":"sites <- read_xlsx(\"Frogwatch_dataset.xlsx\",                     sheet = \"sites\") |>   janitor::clean_names()  sites |> rmarkdown::paged_table()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"prepare-events-csv","dir":"Articles","previous_headings":"","what":"Prepare events.csv","title":"Standardise an Events dataset","text":"observations spreadsheet organised sample-level, row contains multiple observations one 5-minute audio recording, can create Event-based dataframe sample-level use events.csv. First, let’s assign unique identifier eventID data, requirement Darwin Core Standard. Using set_events() composite_id(), can create new column eventID containing unique ID constructed several types information dataframe. Next ’ll add site information sites spreadsheet. use set_coordinates() assign existing columns use valid Darwin Core Standard column names, add 2 required columns geodeticDatum coordinateUncertaintyInMetres. now dataframe sampling site information, organised sample-level. final step reduce obs_id_site include columns valid column names Event-based datasets. drops frog species columns dataframe. can specify wish use events Darwin Core Archive use_data(), save events csv file default directory data-publish ./data-publish/events.csv.","code":"obs_id <- obs |>   select(site_code, year, any_of(species$abbreviation)) |>   set_events(     eventID = composite_id(sequential_id(), site_code, year)     ) |>   relocate(eventID, .before = 1) # re-position #> ⠙ Checking 1 column: eventID #> ✔ Checking 1 column: eventID [328ms] #>   obs_id #> # A tibble: 3,633 × 11 #>    eventID    site_code  year  cpar  csig limdum limper limtas  lper  lver  ulae #>    <chr>      <chr>     <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> #>  1 00001-AMA… AMA100     2004     1     0      0      0      1     1     0     0 #>  2 00002-AMA… AMA100     2007     1     0      1      0      1     0     0     0 #>  3 00003-AMA… AMA100     2007     1     0      1      0      1     0     0     0 #>  4 00004-AMA… AMA100     2005     1     1      1      0      1     0     0     0 #>  5 00005-AMA… AMA100     2008     1     0      1      0      0     1     0     0 #>  6 00006-AMA… AMA100     2008     1     0      1      0      1     1     0     0 #>  7 00007-AMA… AMA100     2013     1     0      1      0      1     0     0     0 #>  8 00008-AMA… AMA100     2008     1     0      1      0      1     1     0     0 #>  9 00009-AMA… AMA100     2013     1     1      0      0      0     0     0     0 #> 10 00010-AMA… AMA100     2014     1     1      1      0      1     0     0     0 #> # ℹ 3,623 more rows obs_id_site <- obs_id |>   left_join(     select(sites, site_code, latitude, longitude),     join_by(site_code)     ) |>   set_coordinates(     decimalLatitude = latitude,      decimalLongitude = longitude,     geodeticDatum = \"WGS84\",     coordinateUncertaintyInMeters = 30     ) |>   relocate(decimalLatitude, decimalLongitude, .after = eventID) # re-position cols #> ⠙ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #> ⠹ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #> ✔ Checking 4 columns: coordinateUncertaintyInMeters, decimalLatitude, decimalLo… #>   obs_id_site #> # A tibble: 3,633 × 15 #>    eventID   decimalLatitude decimalLongitude site_code  year  cpar  csig limdum #>    <chr>               <dbl>            <dbl> <chr>     <dbl> <dbl> <dbl>  <dbl> #>  1 00001-AM…           -35.2             149. AMA100     2004     1     0      0 #>  2 00002-AM…           -35.2             149. AMA100     2007     1     0      1 #>  3 00003-AM…           -35.2             149. AMA100     2007     1     0      1 #>  4 00004-AM…           -35.2             149. AMA100     2005     1     1      1 #>  5 00005-AM…           -35.2             149. AMA100     2008     1     0      1 #>  6 00006-AM…           -35.2             149. AMA100     2008     1     0      1 #>  7 00007-AM…           -35.2             149. AMA100     2013     1     0      1 #>  8 00008-AM…           -35.2             149. AMA100     2008     1     0      1 #>  9 00009-AM…           -35.2             149. AMA100     2013     1     1      0 #> 10 00010-AM…           -35.2             149. AMA100     2014     1     1      1 #> # ℹ 3,623 more rows #> # ℹ 7 more variables: limper <dbl>, limtas <dbl>, lper <dbl>, lver <dbl>, #> #   ulae <dbl>, coordinateUncertaintyInMeters <dbl>, geodeticDatum <chr> events <- obs_id_site |>   select(     any_of(event_terms())     )  events #> # A tibble: 3,633 × 6 #>    eventID            year decimalLatitude decimalLongitude geodeticDatum #>    <chr>             <dbl>           <dbl>            <dbl> <chr>         #>  1 00001-AMA100-2004  2004           -35.2             149. WGS84         #>  2 00002-AMA100-2007  2007           -35.2             149. WGS84         #>  3 00003-AMA100-2007  2007           -35.2             149. WGS84         #>  4 00004-AMA100-2005  2005           -35.2             149. WGS84         #>  5 00005-AMA100-2008  2008           -35.2             149. WGS84         #>  6 00006-AMA100-2008  2008           -35.2             149. WGS84         #>  7 00007-AMA100-2013  2013           -35.2             149. WGS84         #>  8 00008-AMA100-2008  2008           -35.2             149. WGS84         #>  9 00009-AMA100-2013  2013           -35.2             149. WGS84         #> 10 00010-AMA100-2014  2014           -35.2             149. WGS84         #> # ℹ 3,623 more rows #> # ℹ 1 more variable: coordinateUncertaintyInMeters <dbl> events |> use_data()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"prepare-occurrences-csv","dir":"Articles","previous_headings":"","what":"Prepare occurrences.csv","title":"Standardise an Events dataset","text":"Let’s return obs_id_site, contains eventID site information sample. create Occurrence-based dataframe conforms Darwin Core Standard, need transpose wide-format dataframe long format, row contains one observation. ’ll select eventID abbreviated species columns, pivot data species observation abbreviation presence/absence recorded presence. Now ’ll merge correct names frog species joining species obs_long. Now can reformat data use valid Darwin Core column names using set_ functions. Importantly, Darwin Core Standard requires add unique occurrenceID type observation column basisOfRecord. now dataframe observations organised occurrence-level. final step reduce obs_long_dwc include columns valid column names Occurrence-based datasets. drops abbreviation column dataframe. can specify wish use occurrences Darwin Core Archive use_data(), save occurrences csv file default directory data-publish ./data-publish/occurrences.csv.","code":"obs_long <- obs_id_site |>   select(eventID, any_of(species$abbreviation)) |>   pivot_longer(cols = species$abbreviation,                names_to = \"abbreviation\",                values_to = \"presence\")  obs_long #> # A tibble: 29,064 × 3 #>    eventID           abbreviation presence #>    <chr>             <chr>           <dbl> #>  1 00001-AMA100-2004 cpar                1 #>  2 00001-AMA100-2004 csig                0 #>  3 00001-AMA100-2004 limdum              0 #>  4 00001-AMA100-2004 limper              0 #>  5 00001-AMA100-2004 limtas              1 #>  6 00001-AMA100-2004 lper                1 #>  7 00001-AMA100-2004 lver                0 #>  8 00001-AMA100-2004 ulae                0 #>  9 00002-AMA100-2007 cpar                1 #> 10 00002-AMA100-2007 csig                0 #> # ℹ 29,054 more rows obs_long <- obs_long |>   left_join(species, join_by(abbreviation), keep = FALSE) |>   relocate(presence, .after = last_col()) # re-position column obs_long_dwc <- obs_long |>  set_occurrences(    occurrenceID = composite_id(eventID, sequential_id()),    basisOfRecord = \"humanObservation\",    occurrenceStatus = dplyr::case_when(presence == 1 ~ \"present\",                                        .default = \"absent\")    ) |>  set_scientific_name(    scientificName = scientific_name    ) |>  set_taxonomy(    vernacularName = common_name    ) #> ⠙ Checking 3 columns: occurrenceID, basisOfRecord, and occurrenceStatus #> ✔ Checking 3 columns: occurrenceID, basisOfRecord, and occurrenceStatus [919ms] #>  #> ⠙ Checking 1 column: scientificName #> ✔ Checking 1 column: scientificName [311ms] #>  #> ⠙ Checking 1 column: vernacularName #> ⠹ Checking 1 column: vernacularName #> ✔ Checking 1 column: vernacularName [317ms] #>   obs_long_dwc #> # A tibble: 29,064 × 7 #>    eventID           abbreviation occurrenceID    basisOfRecord occurrenceStatus #>    <chr>             <chr>        <chr>           <chr>         <chr>            #>  1 00001-AMA100-2004 cpar         00001-AMA100-2… humanObserva… present          #>  2 00001-AMA100-2004 csig         00001-AMA100-2… humanObserva… absent           #>  3 00001-AMA100-2004 limdum       00001-AMA100-2… humanObserva… absent           #>  4 00001-AMA100-2004 limper       00001-AMA100-2… humanObserva… absent           #>  5 00001-AMA100-2004 limtas       00001-AMA100-2… humanObserva… present          #>  6 00001-AMA100-2004 lper         00001-AMA100-2… humanObserva… present          #>  7 00001-AMA100-2004 lver         00001-AMA100-2… humanObserva… absent           #>  8 00001-AMA100-2004 ulae         00001-AMA100-2… humanObserva… absent           #>  9 00002-AMA100-2007 cpar         00002-AMA100-2… humanObserva… present          #> 10 00002-AMA100-2007 csig         00002-AMA100-2… humanObserva… absent           #> # ℹ 29,054 more rows #> # ℹ 2 more variables: scientificName <chr>, vernacularName <chr> occurrences <- obs_long_dwc |>   select(     any_of(occurrence_terms())     ) occurrences |> use_data()"},{"path":"https://galaxias.ala.org.au/R/articles/events-example.html","id":"summary","dir":"Articles","previous_headings":"Prepare occurrences.csv","what":"Summary","title":"Standardise an Events dataset","text":"hierarchical structure Event-based data (ie Site -> Sample -> Occurrence) adds richness, allowing information like repeated sampling presence/absence information preserved. richness can enable nuanced probabilistic analyses like species distribution models occupancy models. encourage users Event-based data use galaxias standardise data publication sharing.","code":""},{"path":"https://galaxias.ala.org.au/R/articles/occurrences-example.html","id":"the-dataset","dir":"Articles","previous_headings":"","what":"The dataset","title":"Standardise an Occurrences dataset","text":"Let’s use small example dataset bird observations taken 4 different site locations. dataset many different types data like landscape type age class. Importantly standardising Darwin Core, dataset contains scientific name (species), coordinate location (lat & lon) date observation (date).","code":"library(galaxias) library(dplyr) library(readxl)  obs <- read_xlsx(\"dummy-dataset-sb.xlsx\",                  sheet = 1) |>   janitor::clean_names()  obs |>    gt::gt() |>   gt::opt_interactive(page_size_default = 5)"},{"path":"https://galaxias.ala.org.au/R/articles/occurrences-example.html","id":"standardise-to-darwin-core","dir":"Articles","previous_headings":"","what":"Standardise to Darwin Core","title":"Standardise an Occurrences dataset","text":"determine need standardise dataset, let’s use suggest_workflow(). output tells us one matching Darwin Core term data already (sex), missing minimum required Darwin Core terms. “Suggest workflow”, output suggests series piped set_ functions can use rename, modify add columns missing obs required Darwin Core. set_ functions specialised wrappers around dplyr::mutate(), additional functionality support using Darwin Core Standard. simplicity, let’s easy part first renaming columns already dataset use accepted standard Darwin Core terms. set_ functions automatically check make sure column correctly formatted. ’ll save modified dataframe obs_dwc. Running suggest_workflow() reflect progress show us ’s left . Now output tells us still need add several columns dataset meet minimum Darwin Core requirements. ’s rundown columns need add: occurrenceID: Unique identifiers record. ensures can identify specific record future updates corrections. can use composite_id(), sequential_id() random_id() add unique IDs row. basisOfRecord: type record (e.g. human observation, specimen, machine observation). See list acceptable values corella::basisOfRecord_values(). geodeticDatum: Coordinate Reference System (CRS) projection data (example, CRS Google Maps “WGS84”). coordinateUncertaintyInMeters: area uncertainty around observation. may know value based method data collection Now let’s add columns using set_occurrences() set_coordinates(). can also add suggested function set_individual_traits() automatically identify matched column name sex check column’s format. Running suggest_workflow() confirm dataset ready used Darwin Core Archive! submit dataset, let’s select columns valid occurrence term names save dataframe file occurrences.csv. Importantly, save csv folder called data-processed, galaxias looks automatically building Darwin Core Archive. final step save ‘publishing’ directory: done! See Quick start guide vignette build Darwin Core Archive.","code":"obs |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 1 of 12 column names to DwC terms: #> ✔ Matched: sex #> ✖ Unmatched: age_class, comments, date, landscape, lat, lon, molecular_sex, #>   sample_id, site, species, species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)  Missing term(s)                                                                 #> ✖ Identifier (at least one) -                occurrenceID, catalogNumber, recordNumber                                        #> ✖ Record type               -                basisOfRecord                                                                    #> ✖ Scientific name           -                scientificName                                                                   #> ✖ Location                  -                decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters  #> ✖ Date/Time                 -                eventDate #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_datetime() |> #>   set_coordinates() |> #>   set_scientific_name() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs |>   set_scientific_name(scientificName = species) |>   set_coordinates(decimalLatitude = lat,                   decimalLongitude = lon) |>   set_datetime(eventDate = lubridate::ymd(date)) # specify year-month-day format #> ⠙ Checking 1 column: scientificName #> ✔ Checking 1 column: scientificName [320ms] #>  #> ⠙ Checking 2 columns: decimalLatitude and decimalLongitude #> ⠹ Checking 2 columns: decimalLatitude and decimalLongitude #> ✔ Checking 2 columns: decimalLatitude and decimalLongitude [631ms] #>  #> ⠙ Checking 1 column: eventDate #> ✔ Checking 1 column: eventDate [311ms] #> obs_dwc |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 5 of 12 column names to DwC terms: #> ✔ Matched: decimalLatitude, decimalLongitude, eventDate, scientificName, sex #> ✖ Unmatched: age_class, comments, landscape, molecular_sex, sample_id, site, #>   species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)                  Missing term(s)                             #> ✔ Scientific name           scientificName                   -                                            #> ✔ Date/Time                 eventDate                        -                                            #> ✖ Identifier (at least one) -                                occurrenceID, catalogNumber, recordNumber    #> ✖ Record type               -                                basisOfRecord                                #> ✖ Location                  decimalLatitude decimalLongitude geodeticDatum coordinateUncertaintyInMeters #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_coordinates() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs_dwc |>   set_occurrences(     occurrenceID = composite_id(sequential_id(), site, landscape),     basisOfRecord = \"humanObservation\"     ) |>   set_coordinates(     geodeticDatum = \"WGS84\",     coordinateUncertaintyInMeters = 30     # coordinateUncertaintyInMeters = with_uncertainty(method = \"phone\")     ) |>   set_individual_traits() #> ⠙ Checking 2 columns: occurrenceID and basisOfRecord #> ✔ Checking 2 columns: occurrenceID and basisOfRecord [618ms] #>  #> ⠙ Checking 4 columns: decimalLatitude, decimalLongitude, coordinateUncertaintyI… #> ⠹ Checking 4 columns: decimalLatitude, decimalLongitude, coordinateUncertaintyI… #> ✔ Checking 4 columns: decimalLatitude, decimalLongitude, coordinateUncertaintyI… #>  #> ⠙ Checking 1 column: sex #> ✔ Checking 1 column: sex [311ms] #> obs_dwc |>   suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 9 of 16 column names to DwC terms: #> ✔ Matched: basisOfRecord, coordinateUncertaintyInMeters, decimalLatitude, #>   decimalLongitude, eventDate, geodeticDatum, occurrenceID, scientificName, sex #> ✖ Unmatched: age_class, comments, landscape, molecular_sex, sample_id, site, #>   species_code #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)                                                                 Missing term(s)  #> ✔ Identifier (at least one) occurrenceID                                                                    -                 #> ✔ Record type               basisOfRecord                                                                   -                 #> ✔ Scientific name           scientificName                                                                  -                 #> ✔ Location                  decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters -                 #> ✔ Date/Time                 eventDate                                                                       -                 #>  #>  #> 🥇 All minimum column requirements met! #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> 🥇 Your dataframe is Darwin Core compliant! #> Run checks, or use your dataframe to build a Darwin Core Archive with galaxias: #> df |> #>   check_dataset() #>  #> ── Additional functions #> Based on your matched terms, you can also add to your pipe: #> • `set_individual_traits()` #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms obs_dwc <- obs_dwc |>   select(any_of(occurrence_terms())) # select any matching terms  obs_dwc |>   gt::gt() |>   gt::opt_interactive(page_size_default = 5) # Save in ./data-processed use_data_occurrences(obs_dwc)"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"Quick start guide","text":"existing R project containing data collected course research project. project uses fairly standard folder structure. Let’s see galaxias can help us package data Darwin Core Archive.","code":"├── README.md                        : Description of the repository ├── my-project-name.Rproj            : RStudio project file ├── data-raw                         : Folder to store original/source data ├── data                             : Folder to store cleaned data ├── scripts                          : Folder with analytic coding scripts └── plots                            : Folder containing plots/dataviz"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"use-standardised-data-in-an-archive","dir":"Articles","previous_headings":"","what":"Use standardised data in an archive","title":"Quick start guide","text":"Data wish share data folder. look something like : First, ’ll need standardise data conform Darwin Core Standard. suggest_workflow() can help summarising dataset suggesting steps take. Following advice suggest_workflow(), can use set_ functions standardise my_data. set_ functions work lot like dplyr::mutate(): modify existing columns create new columns. suffix set_ function gives indication type data accepts (e.g. set_coordinates(), set_scientific_name), function arguments valid Darwin Core terms use column names. set_ function also checks make sure column contains valid data according Darwin Core Standard. may noticed added additional columns included advice suggest_workflow() (country, locality, taxonRank, kingdom, family). encourage users specify additional information possible avoid ambiguity data shared. use standardised data Darwin Core Archive, can select columns use valid Darwin Core terms column names. Invalid columns won’t accepted try build Darwin Core Archive. data occurrence-based dataset (row contains information observation level, opposed site/survey level), ’ll select columns match names occurrence_terms(). Now can specify wish use my_data_dwc_occ Darwin Core Archive use_data(), saves dataset data_publish folder correct file name occurrences.csv.","code":"my_data #> # A tibble: 2 × 6 #>   latitude longitude date       time  species                  location_id #>      <dbl>     <dbl> <chr>      <chr> <chr>                    <chr>       #> 1    -35.3      149. 14-01-2023 10:23 Callocephalon fimbriatum ARD001      #> 2    -35.3      149. 15-01-2023 11:25 Eolophus roseicapilla    ARD001 my_data |> suggest_workflow() #>  #> ── Matching Darwin Core terms ────────────────────────────────────────────────── #> Matched 0 of 6 column names to DwC terms: #> ✔ Matched: #> ✖ Unmatched: date, latitude, location_id, longitude, species, time #>  #> ── Minimum required Darwin Core terms ────────────────────────────────────────── #>  #>   Type                      Matched term(s)  Missing term(s)                                                                 #> ✖ Identifier (at least one) -                occurrenceID, catalogNumber, recordNumber                                        #> ✖ Record type               -                basisOfRecord                                                                    #> ✖ Scientific name           -                scientificName                                                                   #> ✖ Location                  -                decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters  #> ✖ Date/Time                 -                eventDate #>  #> ── Suggested workflow ────────────────────────────────────────────────────────── #>  #> To make your data Darwin Core compliant, use the following workflow: #>  #> df |> #>   set_occurrences() |> #>   set_datetime() |> #>   set_coordinates() |> #>   set_scientific_name() #>  #> ── Additional functions #> ℹ See all `set_` functions at #>   http://corella.ala.org.au/reference/index.html#add-rename-or-edit-columns-to-match-darwin-core-terms library(lubridate)  my_data_dwc <- my_data |>   # basic requirements of Darwin Core   set_occurrences(occurrenceID = sequential_id(),                   basisOfRecord = \"humanObservation\") |>    # place and time   set_coordinates(decimalLatitude = latitude,                    decimalLongitude = longitude) |>   set_locality(country = \"Australia\",                 locality = \"Canberra\") |>   set_datetime(eventDate = lubridate::dmy(date),                eventTime = lubridate::hm(time)) |>   # taxonomy   set_scientific_name(scientificName = species,                        taxonRank = \"species\") |>   set_taxonomy(kingdom = \"Animalia\",                family = \"Cacatuidae\")   my_data_dwc  #> # A tibble: 2 × 13 #>   location_id basisOfRecord    occurrenceID decimalLatitude decimalLongitude #>   <chr>       <chr>            <chr>                  <dbl>            <dbl> #> 1 ARD001      humanObservation 01                     -35.3             149. #> 2 ARD001      humanObservation 02                     -35.3             149. #> # ℹ 8 more variables: country <chr>, locality <chr>, eventDate <date>, #> #   eventTime <Period>, scientificName <chr>, taxonRank <chr>, family <chr>, #> #   kingdom <chr> library(dplyr)  my_data_dwc_occ <- my_data_dwc |>   select(any_of(occurrence_terms()))  my_data_dwc_occ ## # A tibble: 2 × 12 ##   basisOfRecord    occurrenceID eventDate  eventTime  country   locality ##   <chr>            <chr>        <date>     <Period>   <chr>     <chr>    ## 1 humanObservation 01           2023-01-14 10H 23M 0S Australia Canberra ## 2 humanObservation 02           2023-01-15 11H 25M 0S Australia Canberra ## # ℹ 6 more variables: decimalLatitude <dbl>, decimalLongitude <dbl>, ## #   scientificName <chr>, kingdom <chr>, family <chr>, taxonRank <chr> use_data(my_data_dwc_occ)"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"add-metadata","dir":"Articles","previous_headings":"","what":"Add metadata","title":"Quick start guide","text":"critical part Darwin Core archive metadata statement: tells users owns data, data collected , uses can put (.e. data licence). get example statement, call use_metadata_template(). default, creates R Markdown template named metadata.Rmd working directory. can edit template include information dataset, specify wish use Darwin Core Archive use_metadata(). converts metadata statement Ecological Meta Language (EML), accepted format metadata Darwin Core Archives, saves eml.xml data-publish folder.","code":"use_metadata_template() use_metadata(\"metadata.Rmd\")"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"build-an-archive","dir":"Articles","previous_headings":"","what":"Build an archive","title":"Quick start guide","text":"end process, folder named data-publish contains least two files: One .csv files containing data (e.g. occurrences.csv, events.csv, multimedia.csv) eml.xml file containing metadata can now run build_archive() build Darwin Core Archive! Running build_archive() first checks whether ‘schema’ document (meta.xml) data-publish folder. machine-readable xml document describes content archive’s data files structure. schema document required file Darwin Core Archive. missing, build_archive() build one. can also build schema document using use_schema(). end process, Darwin Core Archive zip file (dwc-archive.zip) paernt directory. also data-publish folder working directory containing standardised data files (e.g. occurrences.csv), metadata statement EML format (eml.xml), schema document (meta.xml).","code":"build_archive()"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"check-archive","dir":"Articles","previous_headings":"","what":"Check archive","title":"Quick start guide","text":"several ways check whether contents Darwin Core Archive meet Darwin Core Standard. first run local tests files inside local folder directory used build Darwin Core Archive. check_directory() allows us check csv files xml files directory Darwin Core Standard criteria. second check whether complete Darwin Core Archive meets institution’s Darwin Core criteria via API. example, can test archive GBIF’s API tests.","code":"check_directory() # Check against GBIF API check_archive(\"dwc-archive.zip\",               email = \"your-email\",               username = \"your-username\",               password = \"your-password\")"},{"path":"https://galaxias.ala.org.au/R/articles/quick_start_guide.html","id":"publishshare-your-archive","dir":"Articles","previous_headings":"","what":"Publish/share your archive","title":"Quick start guide","text":"final step share completed Darwin Core Archive data infrastructure like Atlas Living Australia. share ALA, can launch data submission process browser calling: function provide option open GitHub issue can attach archive. run galaxias test suite dataset respond soon can. ’d prefer use GitHub, can send file brief description support@ala.org.au.","code":"submit_archive()"},{"path":"https://galaxias.ala.org.au/R/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martin Westgate. Author, maintainer. Shandiya Balasubramaniam. Author. Dax Kellie. Author.","code":""},{"path":"https://galaxias.ala.org.au/R/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Westgate M, Balasubramaniam S, Kellie D (2025). galaxias: Describe, Package, Share Biodiversity Data. R package version 0.1.0, https://galaxias.ala.org.au.","code":"@Manual{,   title = {galaxias: Describe, Package, and Share Biodiversity Data},   author = {Martin Westgate and Shandiya Balasubramaniam and Dax Kellie},   year = {2025},   note = {R package version 0.1.0},   url = {https://galaxias.ala.org.au}, }"},{"path":[]},{"path":"https://galaxias.ala.org.au/R/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Describe, Package, and Share Biodiversity Data","text":"galaxias R package helps users describe, bundle, share biodiversity information using ‘Darwin Core’ data standard. galaxias provides tools R build Darwin Core Archive, zip file containing standardised data metadata accepted global data infrastructures. package mirrors functionality devtools, usethis, dplyr manage data, files, folders. galaxias created Science & Decision Support Team Atlas Living Australia (ALA). package named genus freshwater fish found Southern Hemisphere, predominantly Australia Aotearoa New Zealand. logo shows Spotted Galaxias (Galaxias truttaceus) drawn Ian Brennan. comments, questions, suggestions, please contact us.","code":""},{"path":"https://galaxias.ala.org.au/R/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Describe, Package, and Share Biodiversity Data","text":"package active development, yet available CRAN. can install latest development version GitHub : Load package:","code":"install.packages(\"remotes\") remotes::install_github(\"atlasoflivingaustralia/galaxias\") library(galaxias)"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Describe, Package, and Share Biodiversity Data","text":"galaxias contains tools : Standardise save data use_data(). Convert save metadata statements written R Markdown Quarto EML files use_metadata(). Build Darwin Core Archives sharing publishing using build_archive(). Check files consistency Darwin Core Standard, either locally using check_directory() via API using check_archive(). galaxias part group packages helps users publish data using Darwin Core standard. packages group corella delma, automatically loaded galaxias. corella converts tibbles use standard column names delma converts markdown files xml format.","code":""},{"path":"https://galaxias.ala.org.au/R/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Describe, Package, and Share Biodiversity Data","text":"small example dataset species observations. can standardise data according Darwin Core Standard using set_ functions. can specify wish use standardised data Darwin Core Archive use_data(). saves df_dwc valid file extension right location. Create template metadata statement data. editing, can specify wish use “metadata.Rmd” Darwin Core Archive use_metadata(). converts metadata EML format, saves right location correct filename extension. Build Darwin Core Archive save parent directory working directory. Check whether constructed archive passes Darwin Core Standard criteria. See Quick Start Guide -depth explanation building Darwin Core Archives.","code":"library(tibble)  df <- tibble(   scientificName = c(\"Callocephalon fimbriatum\", \"Eolophus roseicapilla\"),   latitude = c(-35.310, -35.273),    longitude = c(149.125, 149.133),   eventDate = lubridate::dmy(c(\"14-01-2023\", \"15-01-2023\")),   status = c(\"present\", \"present\") )  df #> # A tibble: 2 × 5 #>   scientificName           latitude longitude eventDate  status  #>   <chr>                       <dbl>     <dbl> <date>     <chr>   #> 1 Callocephalon fimbriatum    -35.3      149. 2023-01-14 present #> 2 Eolophus roseicapilla       -35.3      149. 2023-01-15 present df_dwc <- df |>    set_occurrences(occurrenceID = random_id(),                    basisOfRecord = \"humanObservation\",                    occurrenceStatus = status) |>    set_coordinates(decimalLatitude = latitude,                    decimalLongitude = longitude)  df_dwc #> # A tibble: 2 × 7 #>   scientificName          eventDate  basisOfRecord occurrenceID occurrenceStatus #>   <chr>                   <date>     <chr>         <chr>        <chr>            #> 1 Callocephalon fimbriat… 2023-01-14 humanObserva… ff8d9d10-40… present          #> 2 Eolophus roseicapilla   2023-01-15 humanObserva… ff8d9d1a-40… present          #> # ℹ 2 more variables: decimalLatitude <dbl>, decimalLongitude <dbl> use_data(df_dwc) use_metadata_template(\"metadata.Rmd\") use_metadata(\"metadata.Rmd\") build_archive() check_archive()"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"citing-galaxias","dir":"","previous_headings":"","what":"Citing galaxias","title":"Describe, Package, and Share Biodiversity Data","text":"generate citation package version using, can run: current recommended citation : Westgate MJ, Balasubramaniam S & Kellie D (2025) galaxias: Describe, Package, Share Biodiversity Data. R Package version 0.1.0.","code":"citation(package = \"galaxias\")"},{"path":"https://galaxias.ala.org.au/R/index.html","id":"contributors","dir":"","previous_headings":"","what":"Contributors","title":"Describe, Package, and Share Biodiversity Data","text":"Developers contributed galaxias follows (alphabetical order surname): Amanda Buyan (@acbuyan), Fonti Kar (@fontikar), Peggy Newman (@peggynewman) & Andrew Schwenke (@andrew-1234)","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a Darwin Core Archive from a folder — build_archive","title":"Build a Darwin Core Archive from a folder — build_archive","text":"Darwin Core archive zip file containing combination data metadata. build_archive() constructs zip file parent directory. function assumes necessary files pre-constructed, can found inside \"data-publish\" directory additional redundant information. Structurally, build_archive() similar devtools::build(), sense takes repository wraps publication.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a Darwin Core Archive from a folder — build_archive","text":"","code":"build_archive(filename = \"dwc-archive.zip\", overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a Darwin Core Archive from a folder — build_archive","text":"filename name file built parent directory. end .zip. overwrite (logical) existing files overwritten? Defaults FALSE. quiet (logical) Whether suppress messages happening. Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a Darwin Core Archive from a folder — build_archive","text":"return anything; called side-effect building 'Darwin Core Archive' (.e. zip file).","code":""},{"path":"https://galaxias.ala.org.au/R/reference/build_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a Darwin Core Archive from a folder — build_archive","text":"function looks three types objects data-publish directory: Data One csv files named occurrences.csv, events.csv /multimedia.csv. csv files contain data standardised using Darwin Core Standard (see corella::corella-package() details). data.frame/tibble can added correct folder using use_data(). Metadata metadata statement EML format file name eml.xml. Completed metadata statements written markdown .Rmd qmd files can converted saved correct folder using use_metadata(). Create new template use_metadata_template(). Schema 'schema' document xml format file name meta.xml. build_archive() detect whether file present build schema file missing. file can also constructed separately using use_schema().","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether an archive meets the Darwin Core Standard via API — check_archive","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"Check whether specified Darwin Core Archive ready sharing publication, according Darwin Core Standard. check_archive() tests archive - defaulting \"dwc-archive.zip\" users' parent directory - using online validation service. Currently supports validation using GBIF.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"","code":"check_archive(   filename = \"dwc-archive.zip\",   username = NULL,   email = NULL,   password = NULL,   wait = TRUE,   quiet = FALSE )  get_report(   obj,   username = NULL,   password = NULL,   n = 5,   wait = TRUE,   quiet = FALSE )  view_report(x, n = 5)  # S3 method for class 'gbif_validator' print(x, ...)"},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"filename name file parent directory pass validator API, ideally created using build_archive(). username GBIF username. email email address used register gbif.org. password GBIF password. wait (logical) Whether wait completed report API exiting (TRUE, default), try API return result regardless (FALSE). quiet (logical) Whether suppress messages happening. Default set FALSE; .e. messages shown. obj Either object class character containing key uniquely identifies query; object class gbif_validator. returned check_archive() get_report() n Maximum number entries print per file. Defaults 5. x object class gbif_validator. ... Additional arguments, currently ignored.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"check_archive() get_report() return object class gbif_validator workspace. view_report() print.gbif_validator() return anything, called side-effect printing useful information console.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check whether an archive meets the Darwin Core Standard via API — check_archive","text":"Internally, check_archive() POSTs specified archive GBIF validator API calls get_report() retrieve (GET) result. get_report() exported allow user download results later time wish; efficient repeatedly generating queries check_archive() underlying data unchanged. third option simply assign outcome check_archive() get_report() object, call view_report() format result nicely. approach require API calls considerably faster. Note information returned functions provided verbatim institution API, galaxias.","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"Checks files data-publish directory meet Darwin Core Standard. check_directory() runs corella::check_dataset() occurrences.csv events.csv files, delma::check_metadata() eml.xml file, present. check_ functions run tests determine whether data metadata pass Darwin Core Standard criteria.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"","code":"check_directory()"},{"path":"https://galaxias.ala.org.au/R/reference/check_directory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether contents of directory comply with the Darwin Core Standard — check_directory","text":"return anything; called side-effect generating report console.","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Build repositories to share biodiversity data — galaxias-package","title":"Build repositories to share biodiversity data — galaxias-package","text":"galaxias helps users describe, package share biodiversity information using 'Darwin Core' data standard, format used accepted Global Biodiversity Information Facility (GBIF) ' partner nodes. galaxias functionally similar devtools, focus building Darwin Core Archives rather R packages. package named genus freshwater fish.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Build repositories to share biodiversity data — galaxias-package","text":"questions, comments suggestions, please email support@ala.org.au. Prepare information Darwin Core use_metadata_template() Add blank metadata statement template working directory suggest_workflow() Advice standardise data using Darwin Core Standard Add information data-publish directory use_data() Save standardised data use Darwin Core Archive use_metadata() Convert metadata file markdown EML (eml.xml) save use Darwin Core Archive use_schema() Build schema file (meta.xml) given directory save use Darwin Core Archive Build archive check_directory() Check files local Darwin Core directory build_archive() Convert directory Darwin Core Archive check_archive() Check whether archive passes Darwin Core criteria via GBIF API submit_archive() Open browser submit data ALA","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/galaxias-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Build repositories to share biodiversity data — galaxias-package","text":"Maintainer: Martin Westgate martin.westgate@csiro.au Authors: Shandiya Balasubramaniam shandiya.balasubramaniam@csiro.au Dax Kellie dax.kellie@csiro.au","code":""},{"path":"https://galaxias.ala.org.au/R/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. corella suggest_workflow delma use_metadata_template","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit a Darwin Core Archive to the ALA — submit_archive","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"preferred method submitting dataset publication via ALA raise issue 'Data Publication' GitHub Repository, attached archive zip file (constructed using build_archive()) issue. dataset especially large (>100MB), need post publicly accessible location (GitHub release) post link instead. function simply opens new issue users' default browser enable dataset submission.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"","code":"submit_archive(quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"quiet Whether suppress messages happening. Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"return anything workspace; called side-effect opening submission form users' default browser.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"process accepting data publication ALA automated; function initiate evaluation process, result data instantly visible ALA. submission guarantee acceptance, ALA reserves right refuse publish data reveals locations threatened -risk species. mechanism entirely public; data visible others point put webpage. data contains sensitive information, contact us arrange different delivery mechanism.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/submit_archive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit a Darwin Core Archive to the ALA — submit_archive","text":"","code":"if(interactive()){   submit_archive() }"},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Use standardised data in a Darwin Core Archive — use_data","title":"Use standardised data in a Darwin Core Archive — use_data","text":"data conform Darwin Core Standard, use_data() makes easy save data correct place building Darwin Core Archive build_archive(). use_data() --one function accepted data types \"occurrence\", \"event\" \"multimedia\". use_data() attempts detect save correct data type based provided tibble/data.frame. Alternatively, users can call underlying functions use_data_occurrences() use_data_events() specify data type manually.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use standardised data in a Darwin Core Archive — use_data","text":"","code":"use_data(..., overwrite = FALSE, quiet = FALSE)  use_data_occurrences(df, overwrite = FALSE, quiet = FALSE)  use_data_events(df, overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use standardised data in a Darwin Core Archive — use_data","text":"... Unquoted name tibble/data.frame save. overwrite default, use_data_events() overwrite existing files. really want , set TRUE. quiet Whether message happening. Default set FALSE. df tibble/data.frame save.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use standardised data in a Darwin Core Archive — use_data","text":"return anything workspace; called side-effect saving .csv file /data-publish.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use standardised data in a Darwin Core Archive — use_data","text":"function saves data data-publish folder. create folder already present. Data type determined detecting type-specific column names supplied data. Event: (eventID, parentEventID, eventType) Multimedia: yet supported","code":""},{"path":[]},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Use a metadata statement in a Darwin Core Archive — use_metadata","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"metadata statement lists owner dataset, collected, can used (.e. ' licence). function reads converts metadata saved markdown (.md), Rmarkdown (.Rmd) Quarto (.qmd) xml, saves data-publish directory. function convenience wrapper function delma::read_md() delma::write_eml().","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"","code":"use_metadata(file = NULL, overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"file metadata file Rmarkdown (.Rmd) Quarto markdown (.qmd) format. overwrite default, use_metadata() overwrite existing files. really want , set TRUE. quiet Whether message happening. Default set FALSE.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"return object workspace; called side effect building file data-publish directory.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"compliant Darwin Core Standard, schema file must called eml.xml, function enforces .","code":""},{"path":[]},{"path":"https://galaxias.ala.org.au/R/reference/use_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use a metadata statement in a Darwin Core Archive — use_metadata","text":"","code":"if (FALSE) { # \\dontrun{ # get a boilerplate metadata statement use_metadata_template(file = \"my_metadata.Rmd\", quiet = TRUE)  # once editting is complete, call `use_metadata()` to format it use_metadata(\"my_metadata.Rmd\") } # }"},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a schema for a Darwin Core Archive — use_schema","title":"Create a schema for a Darwin Core Archive — use_schema","text":"schema xml document maps files field names DwCA. map makes easier reconstruct one related datasets information matched correctly. works detecting column names csv files specified directory; Darwin Core terms function produce reliable results. function assumes publishing directory named \"data-publish\".","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a schema for a Darwin Core Archive — use_schema","text":"","code":"use_schema(overwrite = FALSE, quiet = FALSE)"},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a schema for a Darwin Core Archive — use_schema","text":"overwrite default, use_schema() overwrite existing files. really want , set TRUE. quiet (logical) progress messages suppressed? Default set FALSE; .e. messages shown.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a schema for a Darwin Core Archive — use_schema","text":"return object workspace; called side effect building schema file publication directory.","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a schema for a Darwin Core Archive — use_schema","text":"compliant Darwin Core Standard, schema file must called meta.xml, function enforces .","code":""},{"path":"https://galaxias.ala.org.au/R/reference/use_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a schema for a Darwin Core Archive — use_schema","text":"","code":"if (FALSE) { # \\dontrun{ use_schema() } # }"},{"path":"https://galaxias.ala.org.au/R/news/index.html","id":"galaxias-010","dir":"Changelog","previous_headings":"","what":"galaxias 0.1.0","title":"galaxias 0.1.0","text":"First release version. hope like !","code":""}]
